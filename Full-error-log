[2024-01-13 01:08:25,668] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-13 01:08:25,910] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-13 01:08:25,910] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
DPOConfig(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
beta=0.01,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=2000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=alignment-handbook/zephyr-7b-sft-full,
hub_model_revision=main,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-07,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=data/alignment-handbook/zephyr-7b-sft-full-DPO/runs/Jan13_01-08-25_della-l04g8,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
loss_type=sigmoid,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_length=1024,
max_prompt_length=512,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=paged_adamw_32bit,
optim_args=None,
output_dir=data/alignment-handbook/zephyr-7b-sft-full-DPO,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=data/alignment-handbook/zephyr-7b-sft-full-DPO,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.0,
)
2024-01-13 01:08:25 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/scratch/user/hf/alignment-handbook-zephyr-7b-sft-full/', model_revision='main', model_code_revision=None, torch_dtype=None, trust_remote_code=False, use_flash_attention_2=False, use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
2024-01-13 01:08:25 - INFO - __main__ - Data parameters DataArguments(chat_template=None, dataset_mixer={'/scratch/user/combined_dataset': 1.0}, dataset_splits=['train', 'test'], preprocessing_num_workers=12, truncation_side=None)
2024-01-13 01:08:25 - INFO - __main__ - Training/evaluation parameters DPOConfig(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
beta=0.01,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=2000,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=alignment-handbook/zephyr-7b-sft-full,
hub_model_revision=main,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-07,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=data/alignment-handbook/zephyr-7b-sft-full-DPO/runs/Jan13_01-08-25_della-l04g8,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
loss_type=sigmoid,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_length=1024,
max_prompt_length=512,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=paged_adamw_32bit,
optim_args=None,
output_dir=data/alignment-handbook/zephyr-7b-sft-full-DPO,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=data/alignment-handbook/zephyr-7b-sft-full-DPO,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.0,
)
mixing data

 
 ds = /scratch/user/combined_dataset

 
 ds = /scratch/user/combined_dataset
2024-01-13 01:08:26 - INFO - __main__ - Training on the following splits: ['train : 710187', 'test : 38915']


 model_args = ModelArguments(base_model_revision=None, model_name_or_path='/scratch/user/hf/alignment-handbook-zephyr-7b-sft-full/', model_revision='main', model_code_revision=None, torch_dtype=None, trust_remote_code=False, use_flash_attention_2=False, use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)
 data_args=DataArguments(chat_template=None, dataset_mixer={'/scratch/user/combined_dataset': 1.0}, dataset_splits=['train', 'test'], preprocessing_num_workers=12, truncation_side='left')
2024-01-13 01:08:26 - INFO - __main__ - Prompt sample 670487 of the raw training set:

<|system|>
</s>
<|user|>
Some people say that crystals, have a component of energy that helps with healing. Has any research been done on that?</s>
<|assistant|>
Yes, crystals are sometimes used in healing. Some people think they can help, others think there isn’t any evidence for them. The specific claim that crystals can help you heal is a bit controversial.</s>
<|user|>
I heard that may be the case. Do you have any specific examples of different types of crystals, and how they might help heal?</s>
<|assistant|>
Yes, certain crystals are said to help you heal in certain ways. I recommend reading up on this, since some of it may be controversial. One specific crystal that a lot of people use is rose quartz. It’s also called rose crystal, or rose pink quartz. Some people think it can help you heal, and some think it has no effect. The specific claim is that it can help you feel more calm and peaceful, and it can help your body feel more relaxed. Some people also think it can help you feel energized. It also might help you sleep better.</s>
<|user|>
I've heard similar things about Rose quartz. Same with just clear quartz as well. Any other suggestions?</s>
<|assistant|>
Yes, some people use crystals like crystal cluster sets in their water. People use things like amethyst clusters, clear quartz clusters, and rose quartz clusters, in their water. They think it can help them relax, and they think it can help with their energy levels.</s>
<|user|>
Much like a communion, or baptism in some Christian cults, I figure if it's neutral and it makes someone feel better, why not try.</s>

2024-01-13 01:08:26 - INFO - __main__ - Chosen sample 670487 of the raw training set:

<|assistant|>
Yes, you could try some rose quartz, or some rose quartz cluster, if you think you might like it. I would recommend trying a few pieces, and see if it works for you. This could help you feel more relaxed, and it might</s>

2024-01-13 01:08:26 - INFO - __main__ - Rejected sample 670487 of the raw training set:

<|assistant|>
Yes, that makes sense.</s>

2024-01-13 01:08:26 - INFO - __main__ - Prompt sample 116739 of the raw training set:

<|system|>
</s>
<|user|>
Select the following type of computer language: procedural, object-oriented, or parallel
Java</s>

2024-01-13 01:08:26 - INFO - __main__ - Chosen sample 116739 of the raw training set:

<|assistant|>
Java is an object-oriented programming language. It is well-known for its platform independence, which means it can run on various platforms without needing to be recompiled. The language was designed to be easy to learn and use and has a strong focus on safety and robustness. Java is widely used for web applications, mobile app development (Android), and enterprise-level software development.</s>

2024-01-13 01:08:26 - INFO - __main__ - Rejected sample 116739 of the raw training set:

<|assistant|>
Correct! Java is an object-oriented language.</s>

2024-01-13 01:08:26 - INFO - __main__ - Prompt sample 26225 of the raw training set:

<|system|>
</s>
<|user|>
My kid got lice and I need help to learn how to get rid of them.</s>

2024-01-13 01:08:26 - INFO - __main__ - Chosen sample 26225 of the raw training set:

<|assistant|>
Lice are incredibly common. How did your child get them?</s>

2024-01-13 01:08:26 - INFO - __main__ - Rejected sample 26225 of the raw training set:

<|assistant|>
Okay, have you tried using over-the-counter treatments like Head N’ Shoulders or other treatments recommended by your pediatrician?</s>

[2024-01-13 01:08:27,900] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 7.24B
[2024-01-13 01:08:37,753] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 582, num_elems = 14.48B
2024-01-13 01:08:42 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x145da554bd30>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-01-13 01:08:42,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown
[2024-01-13 01:08:42,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-01-13 01:08:42,873] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
[2024-01-13 01:08:42,954] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-01-13 01:08:42,955] [INFO] [utils.py:803:see_memory_usage] MA 28.48 GB         Max_MA 29.47 GB         CA 29.97 GB         Max_CA 30 GB 
[2024-01-13 01:08:42,955] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.19 GB, percent = 8.3%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-01-13 01:08:43,044] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-01-13 01:08:43,044] [INFO] [utils.py:803:see_memory_usage] MA 28.48 GB         Max_MA 28.48 GB         CA 29.97 GB         Max_CA 30 GB 
[2024-01-13 01:08:43,045] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.19 GB, percent = 8.3%
[2024-01-13 01:08:43,045] [INFO] [config.py:972:print] DeepSpeedEngine configuration:
[2024-01-13 01:08:43,045] [INFO] [config.py:976:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-01-13 01:08:43,045] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-01-13 01:08:43,045] [INFO] [config.py:976:print]   amp_enabled .................. False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   amp_params ................... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   bfloat16_enabled ............. True
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x145d9e430a30>
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   communication_data_type ...... None
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   dataloader_drop_last ......... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   disable_allgather ............ False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   dump_state ................... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... None
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-01-13 01:08:43,046] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   elasticity_enabled ........... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   fp16_auto_cast ............... None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   fp16_enabled ................. False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   global_rank .................. 0
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   grad_accum_dtype ............. None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 1
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   gradient_clipping ............ 0.0
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 1
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   load_universal_checkpoint .... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   loss_scale ................... 1.0
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   memory_breakdown ............. False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   mics_shard_size .............. -1
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   optimizer_name ............... None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   optimizer_params ............. None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   pld_enabled .................. False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   pld_params ................... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   prescale_gradients ........... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   scheduler_name ............... None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   scheduler_params ............. None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   sparse_attention ............. None
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   steps_per_print .............. inf
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   train_batch_size ............. 4
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  4
[2024-01-13 01:08:43,047] [INFO] [config.py:976:print]   use_node_local_storage ....... False
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   weight_quantization_config ... None
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   world_size ................... 1
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  False
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   zero_enabled ................. True
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True
[2024-01-13 01:08:43,048] [INFO] [config.py:976:print]   zero_optimization_stage ...... 3
[2024-01-13 01:08:43,048] [INFO] [config.py:962:print_user_config]   json = {
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_optimization.reduce_bucket_size": 1.677722e+07, 
    "zero_optimization.stage3_param_persistence_threshold": 4.096000e+04, 
    "zero_optimization.stage3_prefetch_bucket_size": 1.509949e+07
}
[2024-01-13 01:08:43,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown
[2024-01-13 01:08:43,821] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-01-13 01:08:43,822] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-01-13 01:08:43,822] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-01-13 01:08:43,831] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-01-13 01:08:43,831] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>
[2024-01-13 01:08:43,831] [WARNING] [engine.py:1157:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2024-01-13 01:08:43,831] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-01-13 01:08:43,831] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-01-13 01:08:43,935] [INFO] [utils.py:802:see_memory_usage] Stage 3 initialize beginning
[2024-01-13 01:08:43,935] [INFO] [utils.py:803:see_memory_usage] MA 27.98 GB         Max_MA 28.49 GB         CA 29.48 GB         Max_CA 30 GB 
[2024-01-13 01:08:43,935] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.19 GB, percent = 8.3%
[2024-01-13 01:08:43,937] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000
[2024-01-13 01:08:43,937] [INFO] [stage3.py:127:__init__] Prefetch bucket size 50,000,000
[2024-01-13 01:08:44,027] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-01-13 01:08:44,028] [INFO] [utils.py:803:see_memory_usage] MA 27.98 GB         Max_MA 27.98 GB         CA 29.48 GB         Max_CA 29 GB 
[2024-01-13 01:08:44,028] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.19 GB, percent = 8.3%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-01-13 01:08:44,131] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-01-13 01:08:44,131] [INFO] [utils.py:803:see_memory_usage] MA 27.98 GB         Max_MA 27.98 GB         CA 29.48 GB         Max_CA 29 GB 
[2024-01-13 01:08:44,132] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.19 GB, percent = 8.3%
[2024-01-13 01:08:44,222] [INFO] [utils.py:802:see_memory_usage] Before creating fp16 partitions
[2024-01-13 01:08:44,223] [INFO] [utils.py:803:see_memory_usage] MA 27.98 GB         Max_MA 27.98 GB         CA 29.48 GB         Max_CA 29 GB 
[2024-01-13 01:08:44,223] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.19 GB, percent = 8.3%
[2024-01-13 01:08:49,073] [INFO] [utils.py:802:see_memory_usage] After creating fp16 partitions: 7
[2024-01-13 01:08:49,075] [INFO] [utils.py:803:see_memory_usage] MA 27.98 GB         Max_MA 27.98 GB         CA 31.79 GB         Max_CA 32 GB 
[2024-01-13 01:08:49,075] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.2 GB, percent = 8.3%
[2024-01-13 01:08:49,174] [INFO] [utils.py:802:see_memory_usage] Before creating fp32 partitions
[2024-01-13 01:08:49,175] [INFO] [utils.py:803:see_memory_usage] MA 27.98 GB         Max_MA 27.98 GB         CA 31.79 GB         Max_CA 32 GB 
[2024-01-13 01:08:49,175] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.2 GB, percent = 8.3%
[2024-01-13 01:08:49,305] [INFO] [utils.py:802:see_memory_usage] After creating fp32 partitions
[2024-01-13 01:08:49,305] [INFO] [utils.py:803:see_memory_usage] MA 54.96 GB         Max_MA 57.04 GB         CA 64.67 GB         Max_CA 65 GB 
[2024-01-13 01:08:49,306] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.2 GB, percent = 8.3%
[2024-01-13 01:08:49,406] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-01-13 01:08:49,406] [INFO] [utils.py:803:see_memory_usage] MA 54.96 GB         Max_MA 54.96 GB         CA 64.67 GB         Max_CA 65 GB 
[2024-01-13 01:08:49,406] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.27 GB, percent = 8.3%
[2024-01-13 01:08:54,798] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-01-13 01:08:54,799] [INFO] [utils.py:803:see_memory_usage] MA 54.96 GB         Max_MA 59.13 GB         CA 68.85 GB         Max_CA 69 GB 
[2024-01-13 01:08:54,799] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 127.44 GB, percent = 12.7%
[2024-01-13 01:08:54,799] [INFO] [stage3.py:460:_setup_for_real_optimizer] optimizer state initialized
[2024-01-13 01:08:55,773] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-01-13 01:08:55,774] [INFO] [utils.py:803:see_memory_usage] MA 69.38 GB         Max_MA 69.86 GB         CA 74.36 GB         Max_CA 74 GB 
[2024-01-13 01:08:55,774] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 133.01 GB, percent = 13.2%
[2024-01-13 01:08:55,774] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-01-13 01:08:55,774] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-01-13 01:08:55,774] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-01-13 01:08:55,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-01-13 01:08:55,775] [INFO] [config.py:972:print] DeepSpeedEngine configuration:
[2024-01-13 01:08:55,775] [INFO] [config.py:976:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-01-13 01:08:55,775] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-01-13 01:08:55,775] [INFO] [config.py:976:print]   amp_enabled .................. False
[2024-01-13 01:08:55,775] [INFO] [config.py:976:print]   amp_params ................... False
[2024-01-13 01:08:55,775] [INFO] [config.py:976:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   bfloat16_enabled ............. True
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1458ab620160>
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   communication_data_type ...... None
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   dataloader_drop_last ......... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   disable_allgather ............ False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   dump_state ................... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... None
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   elasticity_enabled ........... False
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-01-13 01:08:55,776] [INFO] [config.py:976:print]   fp16_auto_cast ............... None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   fp16_enabled ................. False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   global_rank .................. 0
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   grad_accum_dtype ............. None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 1
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   gradient_clipping ............ 0.0
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 1
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   load_universal_checkpoint .... False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   loss_scale ................... 1.0
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   memory_breakdown ............. False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   mics_shard_size .............. -1
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   optimizer_name ............... None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   optimizer_params ............. None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   pld_enabled .................. False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   pld_params ................... False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   prescale_gradients ........... False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   scheduler_name ............... None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   scheduler_params ............. None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   sparse_attention ............. None
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   steps_per_print .............. inf
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   train_batch_size ............. 4
[2024-01-13 01:08:55,777] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  4
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   use_node_local_storage ....... False
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   weight_quantization_config ... None
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   world_size ................... 1
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   zero_enabled ................. True
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True
[2024-01-13 01:08:55,778] [INFO] [config.py:976:print]   zero_optimization_stage ...... 3
[2024-01-13 01:08:55,778] [INFO] [config.py:962:print_user_config]   json = {
    "train_batch_size": 4, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
